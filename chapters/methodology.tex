This chapter shows the practical details of implementation and training.

\section{Models and training}
All of our models are implemented using the Pytorch library
\cite{paszke_pytorch} version 1.2.0.
We use the implementation and pretrained weights of ResNet
available on the Torchvision library \cite{marcel_torchvision}.
We train our own PSPNet based on the implementation by \citeA{huang_psp}.
All models are trained using a single 12 Gb Nvidia Geforce-GTX 1080 Ti GPU except
for the mixed model, which is trained on a 24 Gb Nvidia Titan RTX.

For training we make a 75\%/25\% train/validation splits of the dataset for
each attribute. We keep the splits fixed for all models, so they all see
and are evaluated on the same data. All models are trained for 40 epochs
and we keep the model with the best validation accuracy on epoch end.

\begin{table}[H]
    \begin{center}
        \caption[Hyper parameters]{Hyper parameters and configurations for each model.}
        \begin{tabular}{|l|r|r|r|r|r|}
            \hline
            \textbf{Parameter/Model} & \textbf{ResNet50} & \textbf{ResnetAttn} & \textbf{SegRank}    & \textbf{SelfSegRank} & \textbf{AttentionSegRank} \\ \hline
            Batch Size     & 32                           & 32                                & 32                         & 32                                   & 32                           \\
            Learning Rate  & $10^{-4}$                    & $10^{-4}$                         & $10^{-4}$                  & $10^{-4}$                            & $10^{-4}$   \\
            Opt. Algorithm & SGD                          & SGD                               & Adam                       & Adam                                 & Adam                         \\
            Finetuning     & Yes                          & Yes                               & No                         & No                                   & Yes                          \\
            Dropout        & 0.3                          & 0.3                               & 0                          & 0                                    & 0.1                          \\
            Semantic Dropout        & N/A                          & N/A                               & 0                          & 0.1                                    & 0.1                          \\
            Weight Decay   & $10^{-5}$                    & $10^{-5}$                         & 0                          & 0                            & 0    \\
            \hline
        \end{tabular}
        \label{tab:hyperparams}
    \end{center}
\end{table}

Baselines are trained with SGD with a momentum of $0.9$ \cite{rumelhart_backprop}
as it provided better results empirically. For segmentation based models we train with
Adam \cite{kingma_adam} and we set $\epsilon$, $\beta_1$ and $\beta_2$ to $10^{-9}$, $0.9$ and $0.98$ respectively.
We use semantic dropout on both models that have segmentation and attention, and add
an equivalent regular dropout layer to the ResNetAttn Baseline for fair comparison.
Weight decay and traditional dropout are used for all baseline models that finetune ResNet weights.
See Table \ref{tab:hyperparams} for details on the training hyperparameters.

\section{Visualization}
\label{sec:methodology_visualization}
For visualization we generate both segmentation and attention images, in this section
we will explain how we generate the attention visualizations. See Appendix \ref{sec:seg_colors} for segmentation.

Recall that our attention matrix $A$ is of dimensions $(hw)\times(hw)$, meaning that we
have for each pixel,the attention that this pixel gives to every other
pixel. To reduce the matrix to a single value per pixel $p$, we average the attention
given by every pixel to $p$. This can be done by
simply taking the column wise mean of $A$, which results in a vector with $hw$ components.
Finally, we reshape this vector to the original input size,
obtaining an attention map $A' \in \mathbb{R}_{[0,1]}^{h \times w}$.

Since in practice, $h$ and $w$ are too small to produce a good quality visualization we resize $A'$ with
bilinear interpolation to the standard size of $224 \times 224$. Finally each image is min max normalized to be in
the $[0, 255]$ interval in order to apply the color gradient that generates the final heatmap.
For sample results see Section \ref{sec:visualization_results}.

The heatmaps serve as an additional model interpretation
tool on a per instance level. For example, showing the visualization for all attributes on
a single image, shows how the models pay attention to different parts of the image for different
attributes.