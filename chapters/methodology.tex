This chapter shows the practical details of implementation and training.

\section{Models and training}
All of our models are implemented using the Pytorch library
\cite{paszke_pytorch} version 1.2.0.
We use the implementation and pretrained weights of ResNet
available on the Torchvision library \cite{marcel_torchvision}.
We train our own PSPNet based on the implementation by \citeA{huang_psp}.
All models are trained using a single 12 Gb Nvidia Geforce-GTX 1080 Ti GPU except
for the mixed model, which is trained on a 24 Gb Nvidia Titan RTX.

For training we make a 75\%/25\% train/validation splits of the dataset for
each attribute. We keep the splits fixed for all models, so they all see
and are evaluated on the same data. All models are trained for 40 epochs
and we keep the model with the best validation accuracy on epoch end.

\begin{table}[H]
    \begin{center}
        \caption[Hyper parameters]{Hyper parameters and configurations for each model.}
        \begin{tabular}{|l|r|r|r|r|r|}
            \hline
            \textbf{Parameter/Model} & \textbf{ResNet50} & \textbf{ResnetAttn} & \textbf{SegRank}    & \textbf{SelfSegRank} & \textbf{AttentionSegRank} \\ \hline
            Batch Size     & 32                           & 32                                & 32                         & 32                                   & 32                           \\
            Learning Rate  & $10^{-4}$                    & $10^{-4}$                         & $10^{-4}$                  & $10^{-4}$                            & $10^{-4}$   \\
            Opt. Algorithm & SGD                          & SGD                               & Adam                       & Adam                                 & Adam                         \\
            Finetuning     & Yes                          & Yes                               & No                         & No                                   & Yes                          \\
            Dropout        & 0.3                          & 0.3                               & 0                          & 0                                    & 0.1                          \\
            Semantic Dropout        & N/A                          & N/A                               & 0                          & 0.1                                    & 0.1                          \\
            Weight Decay   & $10^{-5}$                    & $10^{-5}$                         & 0                          & 0                            & 0    \\
            \hline
        \end{tabular}
        \label{tab:hyperparams}
    \end{center}
\end{table}

Baselines are trained with SGD with a momentum of $0.9$ \cite{rumelhart_backprop}
as it provided better results empirically. For segmentation based models we train with
Adam \cite{kingma_adam} and we set $\epsilon$, $\beta_1$ and $\beta_2$ to $10^{-9}$, $0.9$ and $0.98$ respectively.
We use semantic dropout on both models that have segmentation and attention, and add
an equivalent regular dropout layer to the ResNetAttn Baseline for fair comparison.
Weight decay and traditional dropout are used for all baseline models that finetune ResNet weights.
See table \ref{tab:hyperparams} for details on the training hyperparameters.

\section{Visualization}
\label{sec:methodology_visualization}
For visualization we generate both segmentation and attention images, in this section
we will explain how we generate the attention visualizations, for segmentation see appendix \ref{sec:seg_colors}.

We generate the attention images by first extracting the weight matrix $A$ from the softmax operation
of the attention layer (see equation \ref{eq:attention}). $A$  is a square matrix of size
$|A|=(hw)^2$ with $h$ and $w$ the height and width of the layer's input, since it represents
the importance of each pixel for each output of the layer. To reduce it to a single weight per pixel
we take the column wise (opposed dimension of the softmax)  mean of $A$ and
then reshape it to the original image size, obtaining an attention map $A' \in \mathbb{R}_{[0,1]}^{h \times w}$.

Since in practice, $h$ and $w$ are too small to produce a good quality visualization we resize $A'$ with
bilinear interpolation to the standard size of $224 \times 224$. Finally each image is min max normalized to be in
the $[0, 255]$ interval in order to apply the color gradient that generates the final heatmap.
For sample results see section \ref{sec:visualization_results}.