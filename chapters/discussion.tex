In this chapter we will do further analyses of the model results with particular
emphasis on the AttentionSegRank architecture due to it's better performance
and explainability. On section one we will discuss the effects of using semantic segmentation
on neural network  training. Section 2 will show the quantitative relationship between
segmentation, attention and the perception quantification. And finally, section 3 we will
analyze the implications of this method on model explainability.

\section{Effect of semantic segmentation on learning}
As was already mentioned on section \ref{sec:training}, adding a fixed segmentator to
the neural network architecture resulted in a reduction of performance along with a considerable
reduction of overfitting. The behavior was expected when fully replacing the CNN features,
due to the reduced expressiveness of the segmentation and the lack of finetuning, but
unexpectedly, although it is reduced, this behavior persists when combining the fixed
segmentation with the finetuned ResNet50 through the attention layer. We conclude from this
that restricting the attention weights to the shapes and classes given by the segmentation
has a regularizing effect on learning, reducing the model capacity even when the amount of trainable
weights is maintained.

In the case of the PlacePulse dataset this is not a problem since
all traditional deep models suffer of significant overfitting. It remains an interesting research
question  if these behaviors will transfer to other tasks and datasets.

\begin{figure}[ht]
	\begin{center}
	\includegraphics[width=1\textwidth]{./figures/wealthy_graph.png}
	\caption[Wealthy Training curves]{
        Wealthy accuracy vs epoch learning curves on training (a) and validation (b).
        }
	\label{fig:wealthy_graph}
	\end{center}
\end{figure}

\section{Segmentation as an attention subject.}
\label{sec:significance}

By combining the outputs of the segmentation and the attention, we can infer if a certain class was
important for the quantification of perception of a single image. We do this by computing the ratio between
the percentage of pixels belonging to a single class and the percentage of attention placed by the model on
those pixels:
\begin{equation}
	\text{significance ratio} = \frac{\% \text{ attention}}{ \% \text{ segmentation}}
\end{equation}

Using that we define that a class is significant in an image when significance ratio $\geq 1$.
Meaning that the total attention received by the pixels belonging to that class is larger
than the amount they would get if the attention was distributed uniformly over the whole image.

We calculate the significance of all the CityScapes classes
on all the images of the validation split for each of the perception attributes. We show
the results for the most significant classes on figures \ref{fig:total_sig} and \ref{fig:presence_sig}, which show the percentage
of images where each class is considered significant, over the whole split or filtered by the images
where the class is present respectively. For the full results see appendix \ref{sec:sig_tables} .

\begin{figure}[ht]
	\begin{center}
	\includegraphics[width=1\textwidth]{./figures/total_significance.png}
	\caption[Percentage of class significance when total]{
		Percentage of times a segmentation class is considered significant over the whole dataset, for the
		10 most significant classes on average.
        }
	\label{fig:total_sig}
	\end{center}
\end{figure}



\begin{figure}[ht]
	\begin{center}
	\includegraphics[width=1\textwidth]{./figures/present_significance.png}
	\caption[Percentage of class significance when present]{
		Percentage of times a segmentation class is considered significant when it is present in an image, for the
		10 most significant classes on average.
        }
	\label{fig:presence_sig}
	\end{center}
\end{figure}


The most notable insight from this results is that there is considerable differences on the same class
but for different attributes, for example buildings are the most significant for the boring attribute
but is one of the least for the depressing attribute. Vegetation is significant for beautiful and
depressive but not for wealthy. Another insight is that some classes are on average considerably more significant
than others, we believe this happens because they are much more common than others in the dataset
and the networks learn to consider them more often. For the exact distribution of the segmentation classes
see appendix \ref{sec:seg_distribution}.

This results support that attention weights are a good way to augment the explainability of
the models since they show that the same architecture learns to attend to different things
when quantifying different attributes.

We can also visualize which classes are significant for individual examples, allowing
for a simple interpretation of results on a per instance basis.
See figure \ref{fig:ratio} in which the road and sky classes are significant.

\begin{figure}[ht]
	\begin{center}
	\includegraphics[width=1\textwidth]{./figures/ratio.png}
	\caption[Example of significance ratio]{
        Sample visualization including class significance for the safety attribute.
        }
	\label{fig:ratio}
	\end{center}
\end{figure}


\section{Relationship between urban perception and semantic segmentation}

Similarly to previous work \cite{rossetti, zhang_measuring} we analyze how the segmentation relate with the quantified
perception scores, for that we count the percentage of pixels of each segmentation class
on all the images of the validation split for each of the perception attributes and measure
correlation with the output.

Acá van los graficos del score respecto a la segmentación. y la tabla de correlación

Acá van los comentarios de que cosas son importantes para que atributo y comparamos con rosetti ramirez y zhang.

\section{Effect of attention over semantic segmentation on model explainability}
Acá hablamos de las visualizaciones de atención promedio.

Acá mostramos ejemplos de visualizaciones en que se vean atenciones claras junto con el score.